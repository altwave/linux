// SPDX-License-Identifier: GPL-2.0
//#include "vmlinux.h"
#include <string.h>
#include <stdlib.h>
#include <stddef.h>
#include <unistd.h>
#include <linux/bpf.h>
#include <linux/types.h>
//#include <bpf/bpf.h>
//#include <bpf/libbpf.h>
#include <bpf/bpf_helpers.h>
#include <bpf/bpf_tracing.h>
#include <sys/types.h>
#include "bpf_hmm_policy_helpers.h"

char _license[] SEC("license") = "GPL";

#define BPF_STRUCT_OPS(name, args...) \
SEC("struct_ops/"#name) \
BPF_PROG(name, args)


struct hmm_pfn_map {
	__uint(type, BPF_MAP_TYPE_ARRAY);
	__uint(max_entries, 10);
	__type(key, unsigned long);
	__type(value, unsigned long);
};

struct inner_map {
	__uint(type, BPF_MAP_TYPE_ARRAY);
	__uint(max_entries, 10);
	__type(key, int);
	__type(value, unsigned long);
} inner_map1 SEC(".maps");

struct outer_hash {
	__uint(type, BPF_MAP_TYPE_HASH_OF_MAPS);
	__uint(max_entries, 5);
	__uint(key_size, sizeof(int));
	/* Here everything works flawlessly due to reuse of struct inner_map
	 * and compiler will complain at the attempt to use non-inner_map
	 * references below. This is great experience.
	 */
	__array(values, struct inner_map);
} outer_hash SEC(".maps");

//= {
//	.values = { [0] = &inner_map1 },
//};


struct bpf_map_def SEC("maps") hmm_range_map = {
	.type			= BPF_MAP_TYPE_HASH,
	.key_size		= sizeof(unsigned long),
	.value_size		= sizeof(unsigned long),
	.max_entries		= 1024,
};

static inline pmd_t pmd_read_atomic(pmd_t *pmdp)
{
	/*
	 * 	 * Depend on compiler for an atomic pmd read. NOTE: this is
	 * 	 	 * only going to work, if the pmdval_t isn't larger than
	 * 	 	 	 * an unsigned long.
	 * 	 	 	 	 */
	return *pmdp;
}

static inline pmdval_t native_pmd_val(pmd_t pmd)
{
	return pmd.pmd;
}

static inline pudval_t native_pud_val(pud_t pud)
{
	return pud.pud;
}

static inline pudval_t pud_pfn_mask(pud_t pud)
{
	if (native_pud_val(pud) & _PAGE_PSE)
		return PHYSICAL_PUD_PAGE_MASK;
	else
		return PTE_PFN_MASK;
}

static inline pmdval_t pmd_pfn_mask(pmd_t pmd)
{
	if (native_pmd_val(pmd) & _PAGE_PSE)
		return PHYSICAL_PMD_PAGE_MASK;
	else
		return PTE_PFN_MASK;
}
static inline pmdval_t pmd_flags_mask(pmd_t pmd)
{
	return ~pmd_pfn_mask(pmd);
}

static inline pmdval_t pmd_flags(pmd_t pmd)
{
	return native_pmd_val(pmd) & pmd_flags_mask(pmd);
}

static inline pudval_t pud_flags_mask(pud_t pud)
{
	return ~pud_pfn_mask(pud);
}

static inline pudval_t pud_flags(pud_t pud)
{
	return native_pud_val(pud) & pud_flags_mask(pud);
}
static inline pteval_t native_pte_val(pte_t pte)
{
	return pte.pte;
}

static inline pteval_t pte_flags(pte_t pte)
{
	return native_pte_val(pte) & PTE_FLAGS_MASK;
}
/*
 * A clear pte value is special, and doesn't get inverted.
 *
 * Note that even users that only pass a pgprot_t (rather
 * than a full pte) won't trigger the special zero case,
 * because even PAGE_NONE has _PAGE_PROTNONE | _PAGE_ACCESSED
 * set. So the all zero case really is limited to just the
 * cleared page table entry case.
 */
static inline bool __pte_needs_invert(u64 val)
{
	return val && !(val & _PAGE_PRESENT);
}

/* Get a mask to xor with the page table entry to get the correct pfn. */
static inline u64 protnone_mask(u64 val)
{
	return __pte_needs_invert(val) ?  ~0ull : 0;
}
static inline unsigned long pte_pfn(pte_t pte)
{
	phys_addr_t pfn = pte_val(pte);
	pfn ^= protnone_mask(pfn);
	return (pfn & PTE_PFN_MASK) >> PAGE_SHIFT;
}

static inline unsigned long pmd_pfn(pmd_t pmd)
{
	phys_addr_t pfn = pmd_val(pmd);
	pfn ^= protnone_mask(pfn);
	return (pfn & pmd_pfn_mask(pmd)) >> PAGE_SHIFT;
}

static inline unsigned long pud_pfn(pud_t pud)
{
	phys_addr_t pfn = pud_val(pud);
	pfn ^= protnone_mask(pfn);
	return (pfn & pud_pfn_mask(pud)) >> PAGE_SHIFT;
}

static inline int pmd_devmap(pmd_t pmd)
{
	return !!(pmd_val(pmd) & _PAGE_DEVMAP);
}

static inline int pmd_bad(pmd_t pmd)
{
	return (pmd_flags(pmd) & ~_PAGE_USER) != _KERNPG_TABLE;
}
static inline int pmd_trans_huge(pmd_t pmd)
{
	return (pmd_val(pmd) & (_PAGE_PSE|_PAGE_DEVMAP)) == _PAGE_PSE;
}
static inline int pud_trans_huge(pud_t pud)
{
	return (pud_val(pud) & (_PAGE_PSE|_PAGE_DEVMAP)) == _PAGE_PSE;
}
static inline int pmd_present(pmd_t pmd)
{
	/*
	 * Checking for _PAGE_PSE is needed too because
	 * split_huge_page will temporarily clear the present bit (but
	 * the _PAGE_PSE flag will remain set at all times while the
	 * _PAGE_PRESENT bit is clear).
	 */
	return pmd_flags(pmd) & (_PAGE_PRESENT | _PAGE_PROTNONE | _PAGE_PSE);
}
static inline int pmd_none(pmd_t pmd)
{
	/* Only check low word on 32-bit platforms, since it might be
	   out of sync with upper half. */
	unsigned long val = native_pmd_val(pmd);
	return (val & ~_PAGE_KNL_ERRATUM_MASK) == 0;
}
static inline int pmd_huge(pmd_t pmd)
{
	return !pmd_none(pmd) &&
		(pmd_val(pmd) & (_PAGE_PRESENT|_PAGE_PSE)) != _PAGE_PRESENT;
}

static inline int pud_huge(pud_t pud)
{
	return !!(pud_val(pud) & _PAGE_PSE);
}
static inline int pud_write(pud_t pud)
{
	return pud_flags(pud) & _PAGE_RW;
}
static inline int pmd_write(pmd_t pmd)
{
	return pmd_flags(pmd) & _PAGE_RW;
}
static inline int pte_write(pte_t pte)
{
	return pte_flags(pte) & _PAGE_RW;
}

static inline int pud_none(pud_t pud)
{
	return (native_pud_val(pud) & ~(_PAGE_KNL_ERRATUM_MASK)) == 0;
}
static inline int pte_protnone(pte_t pte)
{
	return (pte_flags(pte) & (_PAGE_PROTNONE | _PAGE_PRESENT))
		== _PAGE_PROTNONE;
}

static inline int pmd_protnone(pmd_t pmd)
{
	return (pmd_flags(pmd) & (_PAGE_PROTNONE | _PAGE_PRESENT))
		== _PAGE_PROTNONE;
}
static inline int pud_present(pud_t pud)
{
	return pud_flags(pud) & _PAGE_PRESENT;
}
static inline int pud_devmap(pud_t pud)
{
	return !!(pud_val(pud) & _PAGE_DEVMAP);
}
static inline int pte_none(pte_t pte)
{
	return !(pte.pte & ~(_PAGE_KNL_ERRATUM_MASK));
}
static inline int pte_present(pte_t a)
{
	return pte_flags(a) & (_PAGE_PRESENT | _PAGE_PROTNONE);
}
static inline pte_t ptep_get(pte_t *ptep)
{
	return READ_ONCE(*ptep);
}
static inline pte_t huge_ptep_get(pte_t *ptep)
{
	return ptep_get(ptep);
}
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}
static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & pmd_pfn_mask(pmd));
}
static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
}
static inline unsigned swp_type(swp_entry_t entry)
{
	return (entry.val >> SWP_TYPE_SHIFT);
}
static inline int pmd_swp_soft_dirty(pmd_t pmd)
{
	return 0;
}
static inline pmd_t pmd_swp_clear_soft_dirty(pmd_t pmd)
{
	return pmd;
}
static inline swp_entry_t swp_entry(unsigned long type, pgoff_t offset)
{
	swp_entry_t ret;

	ret.val = (type << SWP_TYPE_SHIFT) | (offset & SWP_OFFSET_MASK);
	return ret;
}
static inline int non_swap_entry(swp_entry_t entry)
{
	return swp_type(entry) >= MAX_SWAPFILES;
}
static inline swp_entry_t pmd_to_swp_entry(pmd_t pmd)
{
	swp_entry_t arch_entry;

	if (pmd_swp_soft_dirty(pmd))
		pmd = pmd_swp_clear_soft_dirty(pmd);
	arch_entry = __pmd_to_swp_entry(pmd);
	return swp_entry(__swp_type(arch_entry), __swp_offset(arch_entry));
}
static inline int pte_swp_soft_dirty(pte_t pte)
{
	return pte_flags(pte) & _PAGE_SWP_SOFT_DIRTY;
}
static inline pte_t native_make_pte(pteval_t val)
{
	return (pte_t) { .pte = val };
}

static inline pte_t pte_clear_flags(pte_t pte, pteval_t clear)
{
	pteval_t v = native_pte_val(pte);

	return native_make_pte(v & ~clear);
}
static inline pte_t pte_swp_clear_soft_dirty(pte_t pte)
{
	return pte_clear_flags(pte, _PAGE_SWP_SOFT_DIRTY);
}
static __always_inline int pte_swp_uffd_wp(pte_t pte)
{
	return 0;
}
static __always_inline pte_t pte_swp_clear_uffd_wp(pte_t pte)
{
	return pte;
}
static inline swp_entry_t pte_to_swp_entry(pte_t pte)
{
	swp_entry_t arch_entry;

	if (pte_swp_soft_dirty(pte))
		pte = pte_swp_clear_soft_dirty(pte);
	if (pte_swp_uffd_wp(pte))
		pte = pte_swp_clear_uffd_wp(pte);
	arch_entry = __pte_to_swp_entry(pte);
	return swp_entry(__swp_type(arch_entry), __swp_offset(arch_entry));
}
static inline int is_zero_pfn(unsigned long pfn)
{
	//extern unsigned long zero_pfn;
	//return pfn == zero_pfn;
	return 0; //fix
}
static inline int pte_special(pte_t pte)
{
	return pte_flags(pte) & _PAGE_SPECIAL;
}
static inline bool is_write_device_private_entry(swp_entry_t entry)
{
	return swp_type(entry) == SWP_DEVICE_WRITE;
}

static inline bool is_device_private_entry(swp_entry_t entry)
{
	int type = swp_type(entry);
	return type == SWP_DEVICE_READ || type == SWP_DEVICE_WRITE;
}

static inline pgoff_t swp_offset(swp_entry_t entry)
{
	return entry.val & SWP_OFFSET_MASK;
}

static inline unsigned long device_private_entry_to_pfn(swp_entry_t entry)
{
	return swp_offset(entry);
}
static inline void migration_entry_wait(struct mm_struct *mm, pmd_t *pmd,
				unsigned long address)
{
	// move to bpf call?
	//spinlock_t *ptl = pte_lockptr(mm, pmd);
	//pte_t *ptep = pte_offset_map(pmd, address);
	//__migration_entry_wait(mm, ptep, ptl);
}
static inline int is_migration_entry(swp_entry_t entry)
{
	return swp_type(entry) == SWP_MIGRATION_READ ||
			swp_type(entry) == SWP_MIGRATION_WRITE;
}

static inline int is_pmd_migration_entry(pmd_t pmd)
{
	return !pmd_present(pmd) && is_migration_entry(pmd_to_swp_entry(pmd));
}

static int __always_inline hmm_pfns_fill(unsigned long addr, unsigned long end, 
		struct hmm_range *range, unsigned long cpu_flags)
{
       	unsigned long i = (addr - range->start) >> PAGE_SHIFT;
	int npages = (end - addr) >> PAGE_SHIFT;
	
	bpf_hmm_pfn_update_user(&range->hmm_pfns[i], cpu_flags, npages * sizeof(unsigned long));
	return 0;
}

static int __always_inline hmm_vma_fault(unsigned long addr, unsigned long end,
			unsigned int required_fault, struct mm_walk *walk)
{
	struct hmm_range *range;
	struct vm_area_struct * vma = (struct vm_area_struct *)bpf_get_mm_walk_vma(walk);
        unsigned long vm_flags;
	unsigned int fault_flags = FAULT_FLAG_REMOTE;

	WARN_ON_ONCE(!required_fault);
	//bpf_hmm_update_walk_last(hmm_vma_walk, addr); //hmm_vma_walk->last = addr;

	bpf_probe_read_kernel(&vm_flags, sizeof(vm_flags), &vma->vm_flags);
	
	if (required_fault & HMM_NEED_WRITE_FAULT) {
		if (!(vm_flags & VM_WRITE))
			return -EPERM;
		fault_flags |= FAULT_FLAG_WRITE;
	}

	for (; addr < end; addr += PAGE_SIZE)
		//if (bpf_handle_mm_fault(vma, addr, fault_flags) & VM_FAULT_ERROR)
			return -EFAULT;
	return -EBUSY;
}


static unsigned int __always_inline hmm_pte_need_fault(struct hmm_range * range,
						       unsigned long pfn_req_flags, unsigned long cpu_flags)
{
	if (!range || !pfn_req_flags)
		return 0;

	pfn_req_flags &= range->pfn_flags_mask;
	pfn_req_flags |= range->default_flags;

	/* We aren't ask to do anything ... */
	if (!(pfn_req_flags & HMM_PFN_REQ_FAULT))
		return 0;

	/* Need to write fault ? */
	if ((pfn_req_flags & HMM_PFN_REQ_WRITE) &&
		!(cpu_flags & HMM_PFN_WRITE))
			return HMM_NEED_FAULT | HMM_NEED_WRITE_FAULT;

	/* If CPU page table is not valid then we need to fault */
	if (!(cpu_flags & HMM_PFN_VALID))
		return HMM_NEED_FAULT;
	return 0;
}


static unsigned int __always_inline
hmm_range_need_fault(struct hmm_range * range, 
		unsigned long hmm_pfns[], unsigned long npages,
		unsigned long cpu_flags) 
{
	unsigned int required_fault = 0;
	unsigned long i;
	unsigned long pfn_req_flags;

	if (!((range->default_flags | range->pfn_flags_mask) & HMM_PFN_REQ_FAULT))
		return 0;
	
	for (i = 0; i < npages; ++i) {
		pfn_req_flags = hmm_pfns[i];
	//	required_fault |= hmm_pte_need_fault(range, pfn_req_flags, cpu_flags);
		if (required_fault == HMM_NEED_ALL_BITS) {
			return required_fault;
		}
	}

	return required_fault;
}

static int __always_inline
_hmm_vma_walk_hole(unsigned long addr_kern, unsigned long end_kern, int depth_kern, 
					struct mm_walk *walk) {
	unsigned long key;
	unsigned int required_fault;
	unsigned long i, npages;
	unsigned long *hmm_pfns;
	unsigned long start;
	struct vm_area_struct *vma; 
	struct hmm_range * range;
	unsigned long addr;
	unsigned long end;
	
	bpf_probe_read_kernel(&addr, sizeof(addr), &addr_kern);
	bpf_probe_read_kernel(&end, sizeof(end), &end_kern);
	
	key = (unsigned long)bpf_get_hmm_vma_walk(walk);
	range = (struct hmm_range *)bpf_map_lookup_elem(&hmm_range_map, &key);
	if (!range)
		return 0;

	i = (addr - range->start) >> PAGE_SHIFT;
	npages = (end_kern - addr_kern) >> PAGE_SHIFT;
	
	hmm_pfns = &range->hmm_pfns[i]; //fix map
	
	required_fault = hmm_range_need_fault(range, hmm_pfns, npages, 0);
	vma = (struct vm_area_struct *)bpf_get_mm_walk_vma(walk);
	if (!vma) {
		if (required_fault)
			return -EFAULT;
		return hmm_pfns_fill(addr, end, range, HMM_PFN_ERROR);
	}
	if (required_fault)
		return hmm_vma_fault(addr, end, required_fault, walk);
	
	return hmm_pfns_fill(addr, end, range, 0);
}

int BPF_STRUCT_OPS(hmm_vma_walk_hole, unsigned long addr_kern, unsigned long end_kern, int depth_kern, 
					struct mm_walk *walk) {	
	return bpf_hmm_vma_walk_hole(addr_kern, end_kern, depth_kern, walk);
	//return _hmm_vma_walk_hole(addr_kern, end_kern, depth_kern, walk);
};

static __always_inline 
unsigned long pte_to_hmm_pfn_flags(struct hmm_range *range, pte_t pte)
{
	if (pte_none(pte) 
			|| !pte_present(pte) 
			|| pte_protnone(pte))
		return 0;
	
	return pte_write(pte) ? (HMM_PFN_VALID | HMM_PFN_WRITE) : HMM_PFN_VALID;
}

static __always_inline unsigned long pmd_to_hmm_pfn_flags(struct hmm_range *range, pmd_t pmd)
{
		if (pmd_protnone(pmd))
			return 0;
		return pmd_write(pmd) ? (HMM_PFN_VALID | HMM_PFN_WRITE) : HMM_PFN_VALID;
}

static int __always_inline hmm_vma_handle_pmd(struct mm_walk *walk, unsigned long addr,
					      unsigned long end, unsigned long hmm_pfns[],
					      pmd_t pmd)
{
	/*
	struct hmm_range *range;
	unsigned long pfn, npages, i;
	unsigned int required_fault;
	unsigned long cpu_flags;
	unsigned long key;
       	
	key = (unsigned long)bpf_get_hmm_vma_walk(walk);
	range = (struct hmm_range *)bpf_map_lookup_elem(&hmm_range_map, &key);
	if (!range) return -1;
	
	npages = (end - addr) >> PAGE_SHIFT;
	cpu_flags = pmd_to_hmm_pfn_flags(range, pmd);
	required_fault = hmm_range_need_fault(range, hmm_pfns, npages, cpu_flags);
	if (required_fault)
		return hmm_vma_fault(addr, end, required_fault, walk);

	pfn = pmd_pfn(pmd) + ((addr & ~PMD_MASK) >> PAGE_SHIFT);
//	for (i = 0; addr < end; addr += PAGE_SIZE, i++, pfn++)
//		hmm_pfns[i] = pfn | cpu_flags;
	*/
	return 0;
}

static __always_inline unsigned long pud_to_hmm_pfn_flags(struct hmm_range *range, pud_t pud)
{
		if (!pud_present(pud))
			return 0;
		return pud_write(pud) ? (HMM_PFN_VALID | HMM_PFN_WRITE) : HMM_PFN_VALID;
}

int BPF_STRUCT_OPS(hmm_vma_walk_pud, pud_t *pudp_kern, unsigned long start, unsigned long end, struct mm_walk *walk) {
	return bpf_hmm_vma_walk_pud(pudp_kern, start, end, walk);
	/*
	struct hmm_range *range;
	unsigned long key;
       	
	key = (unsigned long)bpf_get_hmm_vma_walk(walk);
	range = (struct hmm_range *)bpf_map_lookup_elem(&hmm_range_map, &key);
	if (!range) return -1;

	unsigned long addr = start;
	pud_t * pudp; //  = malloc(sizeof(pud_t)); can't use malloc - alloc on kernel-side
	int ret = 0;
	pud_t pud;
	
	//spinlock_t *ptl = pud_trans_huge_lock(pudp, walk->vma);

	//if (!ptl)
	//	return 0;

	walk->action = ACTION_CONTINUE;

	
//	pud = READ_ONCE(*pudp);
	bpf_hmm_to_user(pudp, pudp_kern, sizeof(pud_t));
	pud = READ_ONCE(*pudp);
	if (pud_none(pud)) {
//		bpf_hmm_spin_unlock(ptl);
		return _hmm_vma_walk_hole(start, end, -1, walk);
	}

	if (pud_huge(pud) && pud_devmap(pud)) {
		unsigned long i, npages, pfn;
		unsigned int required_fault;
		unsigned long *hmm_pfns;
		unsigned long cpu_flags;

		if (!pud_present(pud)) {
			//bpf_hmm_spin_unlock(ptl);
			return _hmm_vma_walk_hole(start, end, -1, walk);
		}

		i = (addr - range->start) >> PAGE_SHIFT;
		npages = (end - addr) >> PAGE_SHIFT;
		hmm_pfns = &range->hmm_pfns[i];

		cpu_flags = pud_to_hmm_pfn_flags(range, pud);
		required_fault = hmm_range_need_fault(range, hmm_pfns, npages, cpu_flags);
		
		if (required_fault) {
			//spin_unlock(ptl);
			return hmm_vma_fault(addr, end, required_fault, walk);
		}

		//Instead, save user_ptr at init, 
		pfn = pud_pfn(pud) + ((addr & ~PUD_MASK) >> PAGE_SHIFT);
		for (i = 0; i < npages; ++i, ++pfn)
			hmm_pfns[i] = pfn | cpu_flags;
		goto out_unlock;

	}

	walk->action = ACTION_SUBTREE;

out_unlock:
	//bpf_hmm_spin_unlock(ptl);
	*/
	//return ret;
//	return 0;
};

static int __always_inline thp_migration_supported(void) {
	return 1; //hard-coded for now
}

static int __always_inline hmm_vma_handle_pte(struct mm_walk *walk, unsigned long addr,
				unsigned long end, pmd_t *pmdp, pte_t *ptep,
				unsigned long *hmm_pfn)
{
	return 0;
	//return bpf_hmm_vma_handle_pte(walk, addr, end, pmdp, ptep, hmm_pfn);
/*
	struct hmm_range *range;
	unsigned long key;
       	
	key = (unsigned long)bpf_get_hmm_vma_walk(walk);
	range = (struct hmm_range *)bpf_map_lookup_elem(&hmm_range_map, &key);
	if (!range) return -1;

	unsigned int required_fault;
	unsigned long cpu_flags;
	pte_t pte = *ptep;
	uint64_t pfn_req_flags = *hmm_pfn;

	if (pte_none(pte)) {
		required_fault = hmm_pte_need_fault(range, pfn_req_flags, 0);
		//if (required_fault)
		//	goto fault;
		*hmm_pfn = 0;
		return 0;
	}

	if (!pte_present(pte)) {
		swp_entry_t entry = pte_to_swp_entry(pte);

		if (is_device_private_entry(entry)) {
			cpu_flags = HMM_PFN_VALID;
			if (is_write_device_private_entry(entry))
				cpu_flags |= HMM_PFN_WRITE;
			*hmm_pfn = device_private_entry_to_pfn(entry) | cpu_flags;
			return 0;
		}

		required_fault = hmm_pte_need_fault(range, pfn_req_flags, 0);
		if (!required_fault) {
			*hmm_pfn = 0;
			return 0;
		}

		if (!non_swap_entry(entry))
			goto fault;

		if (is_migration_entry(entry)) {
			pte_unmap(ptep);
			//hmm_vma_walk->last = addr; fix
			migration_entry_wait(walk->mm, pmdp, addr);
			return -EBUSY;
		}

		pte_unmap(ptep);
		return -EFAULT;
	}
	
	cpu_flags = pte_to_hmm_pfn_flags(range, pte);
	required_fault = hmm_pte_need_fault(range, pfn_req_flags, cpu_flags);
	if (required_fault)
		goto fault;
	
	if (pte_special(pte) && !is_zero_pfn(pte_pfn(pte))) {
		if (hmm_pte_need_fault(range, pfn_req_flags, 0)) {
			pte_unmap(ptep);
			return -EFAULT;
		}
		*hmm_pfn = HMM_PFN_ERROR;
		return 0;
	}

	*hmm_pfn = pte_pfn(pte) | cpu_flags;
	return 0;

fault:
	pte_unmap(ptep);
	*/
	/* Fault any virtual address we were asked to fault */
//	return hmm_vma_fault(addr, end, required_fault, walk);
//	return 0;
}

int BPF_STRUCT_OPS(hmm_vma_walk_pmd, pmd_t *pmdp, unsigned long start, unsigned long end, struct mm_walk *walk) {
//	return 0;
	return bpf_hmm_vma_walk_pmd(pmdp, start, end, walk);
	/*
	struct hmm_range *range;
	unsigned long key;
       	
	key = (unsigned long)bpf_get_hmm_vma_walk(walk);
	range = (struct hmm_range *)bpf_map_lookup_elem(&hmm_range_map, &key);
	if (!range) return -1;
	
	unsigned long *hmm_pfns = &range->hmm_pfns[(start - range->start) >> PAGE_SHIFT];
	unsigned long npages = (end - start) >> PAGE_SHIFT;
	unsigned long addr = start;
	pte_t *ptep;
	pmd_t pmd;

again:
	pmd = READ_ONCE(*pmdp); //todo fix
       	
	if (pmd_none(pmd))
		return _hmm_vma_walk_hole(start, end, -1, walk);

	if (thp_migration_supported() && is_pmd_migration_entry(pmd)) {
		if (hmm_range_need_fault(range, hmm_pfns, npages, 0)) {
			//hmm_vma_walk->last = addr; // fix
			// pmd_migration_entry_wait(walk->mm, pmdp); move to bpf call?
			return -EBUSY;
		}
		return hmm_pfns_fill(start, end, range, 0);
	}

	if (!pmd_present(pmd)) {
		if (hmm_range_need_fault(range, hmm_pfns, npages, 0))
			return -EFAULT;
		return hmm_pfns_fill(start, end, range, HMM_PFN_ERROR);
	}

	if (pmd_devmap(pmd) || pmd_trans_huge(pmd)) {
		// comments omitted
		pmd = pmd_read_atomic(pmdp);
		barrier();
		if (!pmd_devmap(pmd) && !pmd_trans_huge(pmd))
				goto again;
		
		return hmm_vma_handle_pmd(walk, addr, end, hmm_pfns, pmd);
	}
	if (pmd_bad(pmd)) {
		if (hmm_range_need_fault(range, hmm_pfns, npages, 0))
			return -EFAULT;
		return hmm_pfns_fill(start, end, range, HMM_PFN_ERROR);
	}

	ptep = pte_offset_map(pmdp, addr);
//	for (; addr < end; addr += PAGE_SIZE, ptep++, hmm_pfns++) {
//		int r;

//		r = hmm_vma_handle_pte(walk, addr, end, pmdp, ptep, hmm_pfns);
//		if (r) {
//			return r;
//		}
//	}
	pte_unmap(ptep - 1);
*/	
//	return 0;
};

int BPF_STRUCT_OPS(hmm_vma_walk_hugetlb_entry, pte_t *pte, unsigned long hmask,
					unsigned long start, unsigned long end, 
					struct mm_walk *walk) {	
	/*
	unsigned long addr = start, i, pfn;
	unsigned int required_fault;
	unsigned long pfn_req_flags;
	unsigned long cpu_flags;
	struct spinlock_t *ptl;
	pte_t entry;
	struct hmm_range * range;
	struct hmm_vma_walk * hmm_vma_walk;
	unsigned int key;
	
	key = (unsigned long)bpf_get_hmm_vma_walk(walk);
	range = (struct hmm_range *)bpf_map_lookup_elem(&hmm_range_map, &key);
	if (!range)
		return -1;

	struct vm_area_struct *vma = (struct vm_area_struct *)bpf_get_mm_walk_vma(walk);

	ptl = bpf_huge_pte_lock(vma, walk, pte);
	entry = huge_ptep_get(pte);

	i = (start - range->start) >> PAGE_SHIFT;
	pfn_req_flags = range->hmm_pfns[i];
	cpu_flags = pte_to_hmm_pfn_flags(range, entry);
	required_fault = hmm_pte_need_fault(range, pfn_req_flags, cpu_flags);
	if (required_fault) {
		bpf_hmm_spin_unlock(ptl);
		return hmm_vma_fault(addr, end, required_fault, walk);
	}

	pfn = pte_pfn(entry) + ((start & ~hmask) >> PAGE_SHIFT);
	//for (; addr < end; addr += PAGE_SIZE, i++, pfn++)
	//	range->hmm_pfns[i] = pfn | cpu_flags;

	bpf_hmm_spin_unlock(ptl);
*/
//	return 0;
	return bpf_hmm_vma_walk_hugetlb_entry((void *)pte, hmask, start, end, walk);
};

SEC("struct_ops/hmm_test_walk")
int BPF_PROG(hmm_test_walk, unsigned long start_kern, unsigned long end_kern, struct mm_walk *walk) {
	/*struct hmm_vma_walk *hmm_vma_walk;
	struct hmm_range * range;
	unsigned long start = 0;
	unsigned long end = 0;
	unsigned long key, value;
	
	bpf_probe_read_kernel(&start, sizeof(start), &start_kern);
	bpf_probe_read_kernel(&end, sizeof(end), &end_kern);
	
	unsigned long npages = (end_kern - start_kern) >> PAGE_SHIFT;
		
	key = (unsigned long)bpf_get_hmm_vma_walk(walk);
	range = (struct hmm_range *)bpf_map_lookup_elem(&hmm_range_map, &key);
	if (!range)
		return -1;


	// TODO: Fix vma
	struct vm_area_struct * vma = (struct vm_area_struct *)bpf_get_mm_walk_vma(walk);
        unsigned long vm_flags;

	bpf_probe_read_kernel(&vm_flags, sizeof(vm_flags), &vma->vm_flags);
        
	if (!(vm_flags & (VM_IO | VM_PFNMAP | VM_MIXEDMAP)) &&
	    vm_flags & VM_READ)
		return 0;

	
	if (hmm_range_need_fault(range,	
				range->hmm_pfns + ((start_kern - range->start) >> PAGE_SHIFT), 
				npages, 0)) {
		return -EFAULT;
	}
	

	hmm_pfns_fill(start, end, range, HMM_PFN_ERROR);
// deallocate usermem and store into kernel mem
	return 1;*/
	//return 1;
	return bpf_hmm_vma_walk_test(start_kern, end_kern, walk);
//	return 0;
};

SEC("struct_ops/policy_fault")
void BPF_PROG(policy_fault, struct hmm_vma_walk * walk, struct mm_walk_ops * ops) {

	unsigned long key, value;
	unsigned long npages;
	struct inner_map * inner_map;
	struct hmm_range * range;

	key = (unsigned long)walk;
	range = bpf_get_hmm_range_user(walk); //replace wmap
	value = (unsigned long)range;

	bpf_map_update_elem(&hmm_range_map, &key, &value, BPF_ANY);

	int inner_key;
	unsigned long inner_value;
	inner_key = 0;
	inner_value = 75;

//	inner_map = bpf_map_lookup_elem(&outer_hash, &inner_key);
	//if (!inner_map)
	//	return;

//	bpf_map_update_elem(inner_map, &inner_key, &inner_value, 0);

//	bpf_map_update_elem(&outer_hash, &key, &value, BPF_ANY);
       
//	key = 1;

//	npages = (range->end - range->start) >> PAGE_SHIFT;
//	unsigned long pfns[npages] = range->hmm_pfns;

//	if (range && range->hmm_pfns && range->hmm_pfns[0]) 
//		bpf_map_update_elem(&hmm_pfn_map, &key, &range->hmm_pfns[0], BPF_ANY);
	//if (!range) {
	
		//return -1;
//	}
	
	//map = bpf_create_map(BPF_MAP_TYPE_ARRAY, sizeof(key), sizeof(value), npages, 0);
	//for (int i=0; i<npages; i++) {
	//	bpf_map_update_elem(map, &i, &range->hmm_pfns[i], BPF_ANY);
	//}

	bpf_hmm_policy_fault(walk, ops);

	bpf_map_delete_elem(&hmm_range_map, &key);

	// update kern_range with user_range	
	// free user memory
};

SEC(".struct_ops")
struct hmm_policy policy = {
	.fault = (void *)policy_fault,
	.name = "bpf_hmm_policy"
};

SEC(".struct_ops")
struct mm_walk_ops ops = {
	.pud_entry	= (void *)hmm_vma_walk_pud,
	.pmd_entry	= (void *)hmm_vma_walk_pmd,
	.pte_hole	= (void *)hmm_vma_walk_hole,
	.hugetlb_entry	= (void *)hmm_vma_walk_hugetlb_entry,
	.test_walk = (void *)hmm_test_walk,
	.name = "bpf_mm_walk_ops",
};
